{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97c04cc-55e2-417b-841f-b9d51301dc0f",
   "metadata": {},
   "source": [
    "# Exercises in Classification II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257dcc1-2b10-4650-8a84-eda0650f7637",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "In this exercise, we look at the titanic dataset, which is on Moodle in the file \"titanic_survival_data.csv\". \n",
    "\n",
    "Answer the following questions:\n",
    "1. Load in the dataset, replace the missing values in the age column by the mean age of the column, and encode the Sex column as 0 and 1s.\n",
    "2. Make an X set of the variables \"Pclass\", \"Sex\", \"Age\" and \"SibSp\", and take Survived as the y variable. Then make train-test split with 20% of the dataset for testing.\n",
    "3. Do MinMax scaling on the training dataset.\n",
    "5. Use 10-fold cross-validation on the training set to train different KNN algorithms and chose a suitable K based on accuracy score.\n",
    "6. For the chosen K, train a model on the entire training dataset.\n",
    "7. Create a confusion matrix for the model trained in 4 and calculate accuracy, precision, recall, and F1 score on the test dataset.\n",
    "8. OPTIONAL: Create a ROC curve for the test dataset as well as the AUC score\n",
    "9. OPTIONAL: Can you use 10-fold cross validation to get an estimate of the recall instead of accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1c5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac215e5",
   "metadata": {},
   "source": [
    "### 1. Load in the dataset, replace the missing values in the age column by the mean age of the column, and encode the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e886d61a-6163-4cd3-a472-01e9228f9dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Survived  891 non-null    int64  \n",
      " 1   Pclass    891 non-null    int64  \n",
      " 2   Sex       891 non-null    object \n",
      " 3   Age       891 non-null    float64\n",
      " 4   SibSp     891 non-null    int64  \n",
      " 5   Parch     891 non-null    int64  \n",
      " 6   Fare      891 non-null    float64\n",
      " 7   Embarked  891 non-null    object \n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 55.8+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic_data = pd.read_csv(\"titanic_survival_data.csv\")\n",
    "titanic_data.fillna(titanic_data['Age'].mean(), inplace=True)\n",
    "titanic_data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1dd37",
   "metadata": {},
   "source": [
    "### 2. Make an X set of the variables \"Pclass\", \"Sex\", \"Age\" and \"SibSp\", and take Survived as the y variable. Then make train-test split with 20% of the dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81623f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic_data[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\"]]\n",
    "y = titanic_data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "078cb688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonathan Olsen\\AppData\\Local\\Temp\\ipykernel_23108\\2018980599.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Sex'] = X['Sex'].map({'male': 0, 'female': 1})\n"
     ]
    }
   ],
   "source": [
    "X['Sex'] = X['Sex'].map({'male': 0, 'female': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7216de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8532)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4944c",
   "metadata": {},
   "source": [
    "### 3. Do MinMax scaling on the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cc3e426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(X['Sex'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7a2fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36fe7a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.        0.3709909 0.       ]\n",
      " [1.        0.        0.25      0.       ]\n",
      " [0.        0.        0.45      0.       ]\n",
      " ...\n",
      " [0.5       0.        0.425     0.125    ]\n",
      " [1.        0.        0.35      0.       ]\n",
      " [0.        1.        0.65      0.125    ]]\n",
      "\n",
      "\n",
      "[[1.        0.        0.3709909 0.       ]\n",
      " [1.        1.        0.3875    0.       ]\n",
      " [0.        0.        0.4375    0.       ]\n",
      " [1.        1.        0.2625    0.       ]\n",
      " [1.        1.        0.2       0.       ]\n",
      " [1.        1.        0.225     0.       ]\n",
      " [0.        1.        0.275     0.125    ]\n",
      " [0.        0.        0.5       0.       ]\n",
      " [0.5       0.        0.0375    0.125    ]\n",
      " [1.        0.        0.25      0.       ]\n",
      " [1.        1.        0.3709909 0.       ]\n",
      " [0.5       0.        0.3875    0.125    ]\n",
      " [0.        1.        0.3875    0.       ]\n",
      " [1.        1.        0.375     0.       ]\n",
      " [1.        0.        0.225     0.       ]\n",
      " [1.        0.        0.375     0.       ]\n",
      " [1.        0.        0.3709909 0.       ]\n",
      " [1.        0.        0.3       0.       ]\n",
      " [1.        0.        0.3709909 0.       ]\n",
      " [1.        0.        0.05      0.375    ]\n",
      " [0.        1.        0.375     0.       ]\n",
      " [0.5       0.        0.375     0.       ]\n",
      " [1.        0.        0.5       0.       ]\n",
      " [0.        1.        0.75      0.125    ]\n",
      " [1.        0.        0.0125    0.125    ]\n",
      " [0.5       1.        0.3125    0.       ]\n",
      " [0.5       0.        0.2       0.       ]\n",
      " [0.5       0.        0.2625    0.       ]\n",
      " [1.        1.        0.3709909 0.125    ]\n",
      " [1.        0.        0.3       0.       ]\n",
      " [0.5       0.        0.3375    0.       ]\n",
      " [0.        0.        0.675     0.       ]\n",
      " [0.5       0.        0.425     0.       ]\n",
      " [1.        0.        0.925     0.       ]\n",
      " [1.        1.        0.025     0.375    ]\n",
      " [0.5       0.        0.65      0.       ]\n",
      " [0.        0.        0.3709909 0.       ]\n",
      " [1.        0.        0.175     0.5      ]\n",
      " [0.5       0.        0.225     0.       ]\n",
      " [1.        1.        0.0125    0.       ]\n",
      " [1.        0.        0.3709909 0.       ]\n",
      " [1.        0.        0.3709909 0.125    ]\n",
      " [1.        1.        0.05      0.       ]\n",
      " [0.        1.        0.2       0.       ]\n",
      " [0.        0.        0.2125    0.       ]\n",
      " [0.        0.        0.6875    0.       ]\n",
      " [1.        1.        0.1125    0.375    ]\n",
      " [0.5       0.        0.2375    0.       ]\n",
      " [1.        0.        0.2       0.125    ]\n",
      " [1.        0.        0.3709909 1.       ]\n",
      " [0.        0.        0.2625    0.       ]\n",
      " [1.        1.        0.3       0.       ]\n",
      " [0.        0.        0.3709909 0.       ]\n",
      " [0.        0.        0.525     0.       ]\n",
      " [0.        1.        0.4875    0.125    ]\n",
      " [1.        0.        0.6375    0.       ]\n",
      " [0.        1.        0.4875    0.125    ]\n",
      " [1.        0.        0.6375    0.       ]\n",
      " [1.        0.        0.4       0.       ]\n",
      " [1.        0.        0.3625    0.       ]\n",
      " [1.        0.        0.3125    0.       ]\n",
      " [1.        0.        0.6125    0.       ]\n",
      " [1.        1.        0.0625    0.       ]\n",
      " [0.        0.        0.65      0.       ]\n",
      " [1.        0.        0.2625    0.       ]\n",
      " [0.        0.        0.775     0.       ]\n",
      " [0.        0.        0.725     0.       ]\n",
      " [1.        0.        0.3709909 0.       ]\n",
      " [0.5       1.        0.05      0.125    ]\n",
      " [1.        1.        0.3709909 0.       ]\n",
      " [1.        0.        0.5625    0.       ]\n",
      " [1.        0.        0.25      0.       ]\n",
      " [1.        0.        0.2125    0.       ]\n",
      " [1.        1.        0.5625    0.125    ]\n",
      " [0.5       1.        0.275     0.125    ]\n",
      " [1.        0.        0.3709909 0.       ]\n",
      " [1.        0.        0.05      0.125    ]\n",
      " [0.        0.        0.3709909 0.       ]\n",
      " [0.5       0.        0.2875    0.25     ]\n",
      " [1.        0.        0.2875    0.       ]\n",
      " [0.        0.        0.5625    0.       ]\n",
      " [0.5       0.        0.025     0.125    ]\n",
      " [1.        0.        0.2125    0.       ]\n",
      " [0.        0.        0.        0.125    ]\n",
      " [0.        0.        0.1375    0.125    ]\n",
      " [0.        0.        0.3375    0.       ]\n",
      " [1.        1.        0.3709909 0.       ]\n",
      " [1.        0.        0.4       0.       ]\n",
      " [0.        0.        0.7       0.       ]\n",
      " [0.        0.        0.4       0.       ]\n",
      " [1.        0.        0.5875    0.       ]\n",
      " [0.5       0.        0.3625    0.       ]\n",
      " [1.        0.        0.55      0.       ]\n",
      " [1.        0.        0.45      0.125    ]\n",
      " [1.        0.        0.3709909 0.       ]\n",
      " [0.5       0.        0.375     0.125    ]\n",
      " [1.        0.        0.2375    0.       ]\n",
      " [1.        1.        0.        0.25     ]\n",
      " [0.        0.        0.3709909 0.       ]\n",
      " [1.        0.        0.3709909 0.       ]\n",
      " [0.5       1.        0.425     0.       ]\n",
      " [0.        0.        0.3709909 0.       ]\n",
      " [0.5       1.        0.225     0.       ]\n",
      " [1.        0.        0.3709909 0.       ]\n",
      " [0.        1.        0.4375    0.125    ]\n",
      " [1.        0.        0.25      0.125    ]\n",
      " [1.        0.        0.3709909 0.       ]\n",
      " [1.        0.        0.35      0.       ]\n",
      " [1.        0.        0.0875    0.5      ]\n",
      " [0.5       0.        0.0375    0.125    ]\n",
      " [1.        0.        0.2375    0.       ]\n",
      " [1.        0.        0.3709909 0.       ]\n",
      " [0.        1.        0.7       0.       ]\n",
      " [1.        0.        0.3709909 0.       ]\n",
      " [1.        0.        0.55      0.       ]\n",
      " [0.        0.        0.5875    0.       ]\n",
      " [1.        0.        0.375     0.       ]\n",
      " [0.        0.        0.5875    0.       ]\n",
      " [1.        0.        0.3875    0.       ]\n",
      " [1.        1.        0.3709909 0.125    ]\n",
      " [0.        0.        0.6125    0.125    ]\n",
      " [1.        1.        0.7875    0.       ]\n",
      " [0.        0.        0.8875    0.       ]\n",
      " [1.        1.        0.5125    0.       ]\n",
      " [0.        0.        0.3709909 0.       ]\n",
      " [1.        0.        0.4       0.       ]\n",
      " [0.5       1.        0.625     0.       ]\n",
      " [0.5       0.        0.        0.125    ]\n",
      " [1.        0.        0.3709909 0.       ]\n",
      " [1.        0.        0.2625    0.       ]\n",
      " [0.        0.        0.75      0.125    ]\n",
      " [1.        1.        0.3709909 0.       ]\n",
      " [0.        0.        0.3625    0.125    ]\n",
      " [1.        0.        0.325     0.       ]\n",
      " [0.        1.        0.4       0.       ]\n",
      " [1.        0.        0.2       0.       ]\n",
      " [0.5       1.        0.275     0.125    ]\n",
      " [0.        0.        0.3875    0.       ]\n",
      " [1.        0.        0.275     0.       ]\n",
      " [0.        0.        0.375     0.       ]\n",
      " [0.5       0.        0.675     0.       ]\n",
      " [1.        0.        0.275     0.       ]\n",
      " [0.        0.        0.575     0.       ]\n",
      " [0.5       0.        0.225     0.       ]\n",
      " [0.5       0.        0.7375    0.       ]\n",
      " [0.5       1.        0.5       0.       ]\n",
      " [1.        1.        0.1875    0.125    ]\n",
      " [1.        0.        0.1375    0.       ]\n",
      " [1.        1.        0.325     0.       ]\n",
      " [1.        1.        0.3709909 0.       ]\n",
      " [0.        1.        0.5375    0.       ]\n",
      " [0.5       0.        0.2       0.       ]\n",
      " [1.        0.        0.3709909 0.       ]\n",
      " [1.        1.        0.175     0.125    ]\n",
      " [0.        0.        0.3125    0.125    ]\n",
      " [0.        0.        0.3709909 0.       ]\n",
      " [0.        1.        0.4375    0.       ]\n",
      " [1.        1.        0.25      0.125    ]\n",
      " [1.        0.        0.025     0.5      ]\n",
      " [0.        1.        0.475     0.       ]\n",
      " [0.        0.        0.8875    0.       ]\n",
      " [1.        0.        0.2625    0.       ]\n",
      " [1.        0.        0.325     0.       ]\n",
      " [0.        1.        0.675     0.125    ]\n",
      " [0.        0.        0.8       0.       ]\n",
      " [0.5       0.        0.625     0.       ]\n",
      " [0.        0.        0.35      0.125    ]\n",
      " [1.        0.        0.2625    0.       ]\n",
      " [0.5       0.        0.3875    0.125    ]\n",
      " [1.        1.        0.3709909 1.       ]\n",
      " [0.        1.        0.5125    0.       ]\n",
      " [0.5       0.        0.0125    0.       ]\n",
      " [1.        0.        0.875     0.       ]\n",
      " [1.        0.        0.1875    0.125    ]\n",
      " [1.        0.        0.3625    0.       ]\n",
      " [0.        1.        0.2375    0.       ]\n",
      " [0.5       1.        0.3       0.       ]\n",
      " [1.        0.        0.375     0.       ]\n",
      " [1.        0.        0.4125    0.       ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled)\n",
    "print(\"\\n\")\n",
    "print(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03f868",
   "metadata": {},
   "source": [
    "### 5. Use 10-fold cross-validation on the training set to train different KNN algorithms and chose a suitable K based on accuracy score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b24a0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: 3 with accuracy: 0.8174\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define list to store results\n",
    "kacclist = []\n",
    "\n",
    "# Define range of K values to test (different KNN models)\n",
    "for k in range(1, 21):  # Testing K from 1 to 20\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)  # Set the K value\n",
    "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=10, scoring='accuracy')  # 10-fold CV\n",
    "    kacclist.append({\"K\": k, \"Mean accuracy\": scores.mean()})  # Store K and accuracy\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "kaccuracyDF = pd.DataFrame(kacclist)\n",
    "\n",
    "# Find the best K (highest accuracy)\n",
    "best_row = kaccuracyDF.loc[kaccuracyDF[\"Mean accuracy\"].idxmax()]\n",
    "best_k = best_row[\"K\"]\n",
    "best_accuracy = best_row[\"Mean accuracy\"]\n",
    "\n",
    "# Print results\n",
    "print(f\"Best K: {int(best_k)} with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f268ae01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.50000</td>\n",
       "      <td>0.801610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.91608</td>\n",
       "      <td>0.009038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.783725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.75000</td>\n",
       "      <td>0.794308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.50000</td>\n",
       "      <td>0.804108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.25000</td>\n",
       "      <td>0.807996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.00000</td>\n",
       "      <td>0.817449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              K  Mean Accuracy\n",
       "count  20.00000      20.000000\n",
       "mean   10.50000       0.801610\n",
       "std     5.91608       0.009038\n",
       "min     1.00000       0.783725\n",
       "25%     5.75000       0.794308\n",
       "50%    10.50000       0.804108\n",
       "75%    15.25000       0.807996\n",
       "max    20.00000       0.817449"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaccuracyDF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d38529",
   "metadata": {},
   "source": [
    "### 6. For the chosen K, train a model on the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc9537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e091da5",
   "metadata": {},
   "source": [
    "### 7. Create a confusion matrix for the model trained in 4 and calculate accuracy, precision, recall, and F1 score on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d22d08",
   "metadata": {},
   "source": [
    "### 8. OPTIONAL: Create a ROC curve for the test dataset as well as the AUC score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298fc28f",
   "metadata": {},
   "source": [
    "### 9. OPTIONAL: Can you use 10-fold cross validation to get an estimate of the recall instead of accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3abaf15-798d-4cf6-9724-b7df530efe17",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "In this exercise, we will predict the two income classes in the adult dataset (The file \"adult.csv\" is also on Moodle). \n",
    "\n",
    "Answer the following questions:\n",
    "1. Clean the `income` variable such that it has only two values\n",
    "2. Select as set of minimum two feature variables you want to use to predict `income`. Do the necessary transformation of these variables.\n",
    "3. Create X and y dataset and split the datasets into training and testing sets\n",
    "4. Train a KNN classifier to predict the variable `income` based on the feature variables selected in 2 - try out some different Ks \n",
    "5. Train a logistic regression classifier to predict the variable `income` based on the feature variables selected in 2 and compare it to the KNN classifier.\n",
    "6. Train a decision tree classifier to predict the variable `income` based on the feature variables selected in 2 and compare it to the previous classifiers.\n",
    "7. Train a random forest classifier to predict the variable `income` based on the feature variables selected in 2 and compare it to the previous classifiers.\n",
    "8. Train a AdaBoost classifier to predict the variable `income` based on the feature variables selected in 2 and compare it to the previous classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b04df-722a-489c-875a-6b7add99f760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62623518-efd3-4873-97b4-134e5e32aa58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
