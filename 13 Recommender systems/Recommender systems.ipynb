{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e22c86bb-0222-4a1b-bc99-cc19b0a937c8",
   "metadata": {},
   "source": [
    "# Recommender systems\n",
    "\n",
    "In this notebook we will go through various examples of recommender systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd4323-1cee-4dd5-bca9-23c06a75fe78",
   "metadata": {},
   "source": [
    "The code in the notebook is based on the following [DataCamp tutorial](https://www.datacamp.com/tutorial/recommender-systems-python) and uses [The Movies Dataset](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/data) from Kaggle, which is data from IMDB about movies and users. We will only use some of the data that is compressed into a zip on moodle \"TheMovieDataset.zip\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d4878-26cf-4455-ae77-dbeec6fa5340",
   "metadata": {},
   "source": [
    "## Simple recommender system\n",
    "\n",
    "**First, we will do a simple recommender system by simply recommend the Top 250 movies.** \n",
    "\n",
    "For this to work, we have to decide how to rank the movies, which again is done by deciding on a way to assign a score to each movie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c6b06-fe39-413c-8a38-f25c347cd601",
   "metadata": {},
   "source": [
    "For this, let us first look at the meta data about the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b62e5-9476-402e-937c-0fdf897b532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load Movies Metadata\n",
    "metadata = pd.read_csv('movies_metadata.csv', low_memory=False)\n",
    "\n",
    "# Print the first three rows\n",
    "metadata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da08ad9-06aa-4fea-b602-d216e3eb7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159acbe3-e204-47a9-9514-6c7c54ca62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata = metadata.iloc[0:30000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70543d33-15f7-4de7-acea-67ac91c712ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e9a3b-ebd2-446e-9eaa-4b79af4ba55a",
   "metadata": {},
   "source": [
    "Considerations to take into account: The score should not only be based on the average vote, but also on how many that have actually voted on that movie. (Otherwise, a single high vote could make a movie the highest scoring.) Thus, we want a weighted score. For instance:\n",
    "\\begin{equation} \n",
    "\\text Weighted Rating (\\bf WR) = \\left({{\\bf v} \\over {\\bf v} + {\\bf m}} \\cdot R\\right) + \\left({{\\bf m} \\over {\\bf v} + {\\bf m}} \\cdot C\\right)\n",
    "\\end{equation}\n",
    "where $v$ is the number of votes for the movie (`vote_count`), $m$ is the minimum votes required to be listed in the chart, $R$ is the average rating of the movie (`vote_average`), and $C$ is the mean vote across the whole report.\n",
    "\n",
    "$v$ and $R$ we already have in the metadata dataset, and $C$ we can calculate from it. However, $m$ is a hyperparameter we have to choose ourselves.\n",
    "\n",
    "First let us calculate $C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc454e34-b5bf-43b7-a431-2f4176c37754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean of vote average column\n",
    "C = metadata['vote_average'].mean()\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48780446-43d9-4c20-a442-d561c5763d65",
   "metadata": {},
   "source": [
    "For $m$ we will set it at the 90th percentile of number of votes. In that way, we only consider the movies that are in the top 10% in regards to number of votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e162c97-ab5c-4a46-a438-25463a79ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the minimum number of votes required to be in the chart, m\n",
    "m = metadata['vote_count'].quantile(0.90)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842b186-0dc3-43c0-b531-cf82da060e09",
   "metadata": {},
   "source": [
    "We will make a new dataframe `q_movies` that only contains the movies that have more than $m$ (160) number of votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c000fb-1a9c-4a60-b890-860254c00b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all qualified movies into a new DataFrame\n",
    "q_movies = metadata.copy().loc[metadata['vote_count'] >= m]\n",
    "q_movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b21a75-ee99-48a4-8380-ef084eab3758",
   "metadata": {},
   "source": [
    "We will now calculate a weighted ranking of the movies based on the formula above and store it in a new column called `score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20abd9ec-a8da-4278-8d65-8aa549fb681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the weighted rating of each movie\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    # Calculation based on the IMDB formula\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c7cfc-758d-4108-842e-695806ff9b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new feature 'score' and calculate its value with `weighted_rating()`\n",
    "q_movies['score'] = q_movies.apply(weighted_rating, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e92c4-8af4-4726-bbf9-f8786c0e97ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f95ecf8-35ba-441b-8746-c5f370982b5e",
   "metadata": {},
   "source": [
    "Let us sort the dataframe on this new `score` and print the top 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff356b3-0673-4976-81af-b619ef980409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort movies based on score calculated above\n",
    "q_movies = q_movies.sort_values('score', ascending=False)\n",
    "\n",
    "#Print the top 20 movies\n",
    "q_movies[['title', 'vote_count', 'vote_average', 'score']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdec775-b3c6-4dc6-88c2-4c5cc15b2979",
   "metadata": {},
   "source": [
    "We can now recommend new movies to a user based on this `score` - recomming the top movies according to this `score` that the user have not watched yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e2657-8b30-46aa-8686-eb63ea3e8232",
   "metadata": {},
   "source": [
    "## Content-based filtering recommender systems\n",
    "\n",
    "In this section, we will look at Content-based filtering. That is, we will try to recommend movies that are similar in content to movies the user have already watched. The key here is to find a way to represent \"content\" and a way to measure the distance between \"content\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ca53a-67d5-4fac-8d0a-ec336c2c618f",
   "metadata": {},
   "source": [
    "First, we will take the content to be a plot description we actually have in the data. For distance measure, we will use cosine similarity. That is, **we will recommend movies to the user that have plot descriptions, which are similar (measure by cosine similarity) to the plot descriptions of movies the user have already watched.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a42a32-f096-43c4-a25d-247176076fa0",
   "metadata": {},
   "source": [
    "The plot description is available in the variable `overview` of the metadata dataset. let us look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4dd56-b4f3-49b8-b7a8-32381c48c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print plot overviews of the first 5 movies.\n",
    "metadata['overview'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468dc95-5739-4990-94a6-e03a696fdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['overview'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a002f5-3044-45c1-be13-3c26eb65f6de",
   "metadata": {},
   "source": [
    "These plot descriptions are plain text strings and cannot directly be put into any machine learning algorithm. Thus, we have to do some pre-processing to the `overview` variable. As when we looked at IMBD reviews that were labelled as positive or negative in connection with deep learning, we can use one-hot-encoding. That is, we can make a column for each of the most common words and put a 1 if the word is in the plot description and 0 if the word is not in the plot description.\n",
    "\n",
    "This would work, but is a crude encoding. We can do a bit better in the sense that we instead of a 1 can but a score between 0 and 1 that somehow represent the importance of that word. One such importance score is *Term Frequency-Inverse Document Frequency* (TF-IDF). This score note how often the word appears in the given plot description in relation to how often it occurs overall in all the plot descriptions. \n",
    "\n",
    "By \"term\" we just mean word and by \"document\" we mean a plot description. Then we can first calculate the *relative term frequency* of a term in a document - that is, how often a word occurs in a particular plot description. The formula for this is:\n",
    "$$\n",
    "tf(t, d) = \\frac{f_{t, d}}{len(d)} \n",
    "$$\n",
    "where $t$ is the term, $d$ is the document, $f_{t, d}$ is the count of how many times the term $t$ appears in the document $d$, and $len(d)$ is the total count of terms in $d$. \n",
    "\n",
    "In addition, we can define the *inverse document fequency* by the formula:\n",
    "$$\n",
    "idf(t, D) = \\log {\\frac {\\# D}{\\# D_t}}\n",
    "$$\n",
    "where $D$ is the set of all documents (in our case all the plot descriptions), $D_t$ is the set of documents that contain the term $t$, $\\#D$ the number of documents in $D$, and $\\# D_t$ is the number of documents that contain $t$.\n",
    "\n",
    "With relative term frequency and inverse document frequency defined, we can finally define *TF-IDF* as:\n",
    "$$\n",
    "TF-IDF(t, d, D) = tf(t, d) + idf(t, D)\n",
    "$$\n",
    "\n",
    "Luckily, we do not have to calculate these things manually, but can use a build in functionality of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92334c-de33-4a4f-b537-c6bcedbf3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RWe first replace missing values with an empty string\n",
    "metadata['overview'] = metadata['overview'].fillna('')\n",
    "\n",
    "#Import TfIdfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix = tfidf.fit_transform(metadata['overview'])\n",
    "\n",
    "#Output the shape of tfidf_matrix\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c131cc9-e995-4de7-9295-5fbdf469dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.toarray()[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe22521-1c8a-4f4f-8ddf-84b2bcde62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d285c3d6-8ff9-439c-b300-6b281f8e853c",
   "metadata": {},
   "source": [
    "Now that we have each movie represented as a 75827 long vector (the rows), then we just need a way to measure the distance between two such vectors (movies/rows). For this, we will use the cosine similarity, which commonly used for tasks like this. Cosine similarity measure \"the angle\" between two vectors. If the vectors are proportional (have the same direction) the cosine similarity is 1, if the vectors are orthogonal it is 0, and if the vectors are pointing in completely opposite directions it is -1. (The way we constructed our rows, we will never get negative cosine similarity values.) Cosine similarity is also fast to compute for sparse rows like the one we have here (most values are 0). The formula for cosine similarity is:\n",
    "$$\n",
    "cos(A, B) = \\frac{\\Sigma_{i}a_i * b_i}{\\sqrt(\\Sigma_{i}a_i^2)*\\sqrt(\\Sigma_{i}b_i^2)}\n",
    "$$\n",
    "where $A$ and $B$ are vectors (in our case rows) and $a_i$ is the i'th element of the vector $A$ and $b_i$ is the i'th element of the vector $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28cc857-6762-4c9c-af49-2a65643460df",
   "metadata": {},
   "source": [
    "We calculate the cosine similarity between any two movies. We will store this is a matrix (2D array) of shape 45466 x 45466, where each column and row correspond to a movie. In this way, each row will correspond to a movie and the values will be the cosine similarity between that movie and all the other, 45466 (including itself) movies. (The blog argues for using a linear kernel to calculate cosine similarities faster, but we might as well just use the `cosine_similarity` function from scikit-learn - it is often fast enough.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86b9b5-417d-4086-b2fc-6147e726f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06770c9b-4664-4d94-99e0-b5d6c86fc20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c6248-4fea-4684-833d-21eb37e154bf",
   "metadata": {},
   "source": [
    "The linear kernel actually turned out to be slower in this case! Let us remove this matrix (cosine_simLK) as it is quite big and take up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd3012-f14c-440f-a56a-5f0e20be4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0bc0d-7aa7-4d94-adbe-ee16210ab3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f46cdd-3da4-4816-88df-a27d9fc2eeeb",
   "metadata": {},
   "source": [
    "This matrix is symetric in the sense that `cosine_sim[0, 1]` tell us how much the first movie (index 0) is similar to the second movie (index 1), which returns the exact same value as `cosine_sim[1, 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21ce28-307a-4042-b6e3-082be69f9114",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409e9fd-cfb0-416b-b675-1a835fd1aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbdcbb3-d467-4947-a469-7c1f0510c94a",
   "metadata": {},
   "source": [
    "To have any idea if this makes sense, we can look up the corresponding titles in the original metadata dataset. For later use, let us make a reverse map of index to titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9988a1-579d-41fd-9312-d1da59a26a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a reverse map of indices and movie titles\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0f308-a521-4271-90ec-1ed2cf3665f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244ee64-c167-4540-ae06-d7117b789116",
   "metadata": {},
   "source": [
    "We can see that the similarity `cosine_sim[0, 1]` is the similarity between \"Toy Story\" and \"Jumanji\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7d8a3a-cd46-44b7-8470-f56755a9d4cf",
   "metadata": {},
   "source": [
    "We can now define a recommender function, that is, we can define a function that takes in a movie title as input and returns a list of the 10 most similar movies to the input movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff3a58-ac12-4828-ad31-84a3abbdb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in movie title as input and outputs most similar movies\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd6b67-c6ad-4677-89fe-189f33f3852a",
   "metadata": {},
   "source": [
    "We can now try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d10f7b-3847-4e9f-bff6-703145413506",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('Toy Story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451da007-95f1-4d96-a257-4d66391bd36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('The Dark Knight Rises')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11c4b2-dab2-4d28-8736-f970214a0dea",
   "metadata": {},
   "source": [
    "This recommender is not completely off, but still not perfect, of course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf4410b-7380-4765-a4c4-8c3fbed9e5af",
   "metadata": {},
   "source": [
    "## Improved content-based filtering\n",
    "\n",
    "We can **improve the recommender by considering more metadata about the movies, such as staring actors, the director, related genres, and keywords**. First, we load in this additional data and merge it with our original metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456ff2c-1b6b-40e2-af92-6ad716f0d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keywords and credits\n",
    "credits = pd.read_csv('credits.csv')\n",
    "keywords = pd.read_csv('keywords.csv')\n",
    "\n",
    "# Remove rows with bad IDs.\n",
    "metadata = metadata.drop([19730, 29503, 35587])\n",
    "\n",
    "# Convert IDs to int. Required for merging\n",
    "credits['id'] = credits['id'].astype('int')\n",
    "keywords['id'] = keywords['id'].astype('int')\n",
    "metadata['id'] = metadata['id'].astype('int')\n",
    "\n",
    "# Merge keywords and credits into your main metadata dataframe\n",
    "metadata = metadata.merge(credits, on='id')\n",
    "metadata = metadata.merge(keywords, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e00d80-404c-44c9-a01b-c1a1fd9afadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7291df-d55d-42ea-a968-858cf2165726",
   "metadata": {},
   "source": [
    "We can see that our new columns `cast`, `crew`, and `keywords` are some strange format - it looks like JSON in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec2d286-08ba-43aa-a2dd-831e443ad931",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.cast[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018321c-30e0-49a0-9483-43603be4c83f",
   "metadata": {},
   "source": [
    "We can decode it a bit using the `literal_eval` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a3a5b3-1d0d-456a-9acc-19f93dd0c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the stringified features into their corresponding python objects\n",
    "from ast import literal_eval\n",
    "\n",
    "features = ['cast', 'crew', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    metadata[feature] = metadata[feature].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7001500-1174-4697-9cde-40731ebe4ff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata.cast[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09dc29-8359-4960-a327-d74621de5465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata.crew[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f28e79-2423-4ca2-ace5-b60f4ae963f5",
   "metadata": {},
   "source": [
    "We can now build a function that fetches the director, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d39a144-c501-49db-bbc0-92b2d12859e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_director(x):\n",
    "    for i in x:\n",
    "        if i['job'] == 'Director':\n",
    "            return i['name']\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fe4db3-3214-4f88-88e3-8154c6839e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_director(metadata.crew[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2e455-d0f3-4670-8347-42e78c1d4f33",
   "metadata": {},
   "source": [
    "For `cast`, `keywords`, and `genres` we are just goint to retrieve the first 3 (top 3) elements. We can also make a function for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031a567-4364-4e61-bc29-c1a670b9de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(x):\n",
    "    if isinstance(x, list):\n",
    "        names = [i['name'] for i in x]\n",
    "        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n",
    "        if len(names) > 3:\n",
    "            names = names[:3]\n",
    "        return names\n",
    "\n",
    "    #Return empty list in case of missing/malformed data\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3d2fd-f3b7-4684-9948-0ee0d76f0c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list(metadata.cast[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7432a84-6647-4ec7-aa14-47985fb97d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list(metadata.keywords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2228e4a-a47b-485f-8329-178efa87ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list(metadata.genres[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a4d5d-9f78-4dcb-bd0a-a70a4de12cd1",
   "metadata": {},
   "source": [
    "With these helper functions, we can now define new features for director, cast, genres and keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8395a-3310-4b62-97f5-d0925996ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new director, cast, genres and keywords features that are in a suitable form.\n",
    "metadata['director'] = metadata['crew'].apply(get_director)\n",
    "\n",
    "features = ['cast', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    metadata[feature] = metadata[feature].apply(get_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306d83a-5bcb-4ce9-9ec9-52e0c3eca775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the new features of the first 3 films\n",
    "metadata[['title', 'cast', 'director', 'keywords', 'genres']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e88df-5752-42d3-a32c-38b598f28d29",
   "metadata": {},
   "source": [
    "This new metadata about movies is still text data, thus we need to pre-process somehow to make it fit further analysis. There are several options for this, but essentially we want to vectorize the data and to do this it can sometimes be beneficial to combine the data into one string (\"soup\" - I am not sure if this is a commonly used term!) before vectorizing it. The tutorial does this by replacing upper case letters with lower case letters and removing black spaces, before concatenating the text strings into one string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d6a58-7d63-43e8-90c1-bf293de4e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert all strings to lower case and strip names of spaces\n",
    "def clean_data(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
    "    else:\n",
    "        #Check if director exists. If not, return empty string\n",
    "        if isinstance(x, str):\n",
    "            return str.lower(x.replace(\" \", \"\"))\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b1d8a-9bf6-4c89-b7cf-a96b94acd698",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(metadata.cast[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfb7ca-459b-4aab-9618-1bf25dbc0e18",
   "metadata": {},
   "source": [
    "We then apply that function to all the relevant coulmns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0a8e1-b106-43c1-ad9e-557542079ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clean_data function to your features.\n",
    "features = ['cast', 'keywords', 'director', 'genres']\n",
    "\n",
    "for feature in features:\n",
    "    metadata[feature] = metadata[feature].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44c4ef6-28a6-4f7b-a3e7-62554264ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the new features of the first 3 films\n",
    "metadata[['title', 'cast', 'director', 'keywords', 'genres']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b2e99-87af-4a59-83af-b3b11393e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(x):\n",
    "    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c5799-1f39-47f0-a05e-da1f7f1abd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new soup feature\n",
    "metadata['soup'] = metadata.apply(create_soup, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c61b8-b603-4b2d-8150-c7a5aa789483",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.soup[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf6b72-feb3-4569-9b15-34ebc5a4bd8f",
   "metadata": {},
   "source": [
    "For vectorization we will use something else than TF-IDF, since we are not dealing with traditional text documents. Thus we will use the count vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52e64d-b295-4798-8335-928070cd7cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer and create the count matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english')\n",
    "count_matrix = count.fit_transform(metadata['soup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24aaad-8fea-4877-aaa3-267a40ebd719",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32278f6-174f-4673-8e8b-9fa9327ef15a",
   "metadata": {},
   "source": [
    "We will again use the cosine similarity to calculate the difference between the resulting vectors. Be aware that this will put a high load on the memory (and CPU)!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cba512-5f09-4772-b9f3-c197c3f8ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Cosine Similarity matrix based on the count_matrix\n",
    "cosine_sim2 = cosine_similarity(count_matrix, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebef5f2-d067-43fc-b11b-957e61dbe1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index of your main DataFrame and construct reverse mapping as before\n",
    "metadata = metadata.reset_index()\n",
    "indices = pd.Series(metadata.index, index=metadata['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba0869-be3b-435f-b4ae-f01c96dea02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('Toy Story', cosine_sim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c55ffd-98cb-4499-a4bf-aba9ab437264",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('The Dark Knight Rises', cosine_sim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f9754-b362-4def-a0d1-3698934534b8",
   "metadata": {},
   "source": [
    "## An example of User-Based Collaborative Filtering\n",
    "\n",
    "In this section, we will look at Collaborative-based filtering. More specifically, we will make a user-based collaborative filtering based on data about the users (rating the movies). The example is based on the same movie dataset and the following Kaggle notebook: [https://www.kaggle.com/code/yagizcapa/user-based-recommender](https://www.kaggle.com/code/yagizcapa/user-based-recommender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1d6c2-0cd6-4c05-abd0-b8298236ba97",
   "metadata": {},
   "source": [
    "First we read in the rating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c592b1-e4fb-406e-b191-e31f54471ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"ratings_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef6719-b3f9-4727-a20c-cf1ae01de339",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = metadata.merge(ratings, how=\"left\", left_on=\"id\", right_on=\"movieId\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa0845-357f-4739-bc32-6ecaa9c2a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf1175-8f47-4c43-af85-a23ba700a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b8de69-92d0-436a-85b2-717c0ac099dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"userId\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3eb0d-76de-4b6d-a473-20d9670cb4f9",
   "metadata": {},
   "source": [
    "We there are 88822 user ratings by 671 users (of 42276 unique movies - it is not given that all movies have ratings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5661c985-fd15-4352-95ab-abb92ffa9db6",
   "metadata": {},
   "source": [
    "Let us look at how many rated the most rated movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b937160e-00ca-40ff-8dd2-47a8fa5b888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_counts = pd.DataFrame(df[\"title\"].value_counts())\n",
    "\n",
    "rating_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f415eb-f022-4761-9fcc-d4ee99750d83",
   "metadata": {},
   "source": [
    "You might wonder what happens with all the rows (and movies) that did not have rating? We can remove those to ensure that the above code does not count rows where the rating is missing. By doing this, we also learn that there are 2794 movies that are rated by the 671 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3dd0f-22ec-4077-b806-09193ac2d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"title\", \"rating\"]].dropna().drop(columns=[\"rating\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739c34e-41f7-4867-9b64-011910d5d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_df = df[[\"userId\", \"title\", \"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b383ac-617e-4031-a0e2-b494c014c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637894af-cb5b-471b-8455-c54ada1d11cc",
   "metadata": {},
   "source": [
    "We create a dataframe with the user ratings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4dd41-40e6-4cdb-9730-79a8d4dcf749",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_df = user_movie_df.pivot_table(index=[\"userId\"], columns=[\"title\"], values=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5249a-32dc-4471-ba24-3cd733977299",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb07c738-74e8-4ac4-a793-671053a38a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fe487-4b21-4c03-8e98-1c574fe85af7",
   "metadata": {},
   "source": [
    "Let us select a random user as an example case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444924bb-0715-48ad-8c2c-d17c23ef8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_user = np.array(user_movie_df.sample(random_state = 50).index)[0]\n",
    "random_user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab174c6-28b2-4b88-84dc-13c858f7cba1",
   "metadata": {},
   "source": [
    "Getting the movies the random user have rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d2e07c-1c06-4467-87f8-a5f9a4f35e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_user_df = user_movie_df[user_movie_df.index == random_user]\n",
    "random_user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b8154-ef42-42bb-9dbd-e8f961b6501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_user_movies_watched = random_user_df.columns[random_user_df.notna().any()].tolist()\n",
    "random_user_movies_watched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea4f35-aa84-4e32-b90a-d12f765914fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_user_movies_watched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca6e49-d5d7-4b3d-bac4-9d9649062579",
   "metadata": {},
   "source": [
    "Selecting only does movies to look for similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a4b8d-90cd-481d-b090-63e14cacde69",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_watched_df = user_movie_df[random_user_movies_watched]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e37faa-5186-4df9-97d4-b6d7e69223ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_watched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b88468-1ce3-436e-a669-936bc693389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_watched_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c76dbc-430e-45db-b335-e12c7a5f8b24",
   "metadata": {},
   "source": [
    "For each other user, we now calculate how many movies they have rated among these selected that the random user have rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f19a8a8-2dfa-4405-80a4-0f95a84cef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_count = movies_watched_df.T.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a455b52d-9161-4cc5-bd24-1b8816f50dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b71660-140c-45a0-bdc5-f94320cc8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_count = user_movie_count.reset_index()\n",
    "user_movie_count.columns = [\"userId\", \"movie_count\"]\n",
    "user_movie_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99f696c-a1fa-4fe8-b8df-3aa914de230d",
   "metadata": {},
   "source": [
    "We select those users that have rated more than 70% of the movies the random user have rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0282320-5adc-4b9e-80b8-cfba89faf95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_same_movies = user_movie_count[user_movie_count[\"movie_count\"] > (len(random_user_movies_watched)*70)/100][\"userId\"]\n",
    "user_same_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23203fef-00bf-4755-9554-e400b0b128d8",
   "metadata": {},
   "source": [
    "creating a data frame with the rating of only these users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa9b649-bef1-4393-b6b4-51d9d68ea9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = movies_watched_df[movies_watched_df.index.isin(user_same_movies)]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447696e6-33b5-4b42-ab3d-134c1843d529",
   "metadata": {},
   "source": [
    "We now calculate the correlation between all the users. That is the correlation between the rows. As the `.corr` method on data frames calculate the correlations between columns, we have transpose the data frame first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96edf914-4520-4b34-bb3c-aa91f69cfbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = final_df.T.corr()\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9cc67-08c3-4c8b-be1f-62e08f656195",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_corr = corr_df[random_user].reset_index()\n",
    "user_corr = user_corr.rename(columns={random_user: 'correlation'})\n",
    "user_corr = user_corr.sort_values(by=\"correlation\", ascending=False)\n",
    "user_corr = user_corr.loc[user_corr[\"userId\"] != random_user]\n",
    "user_corr = user_corr.reset_index(drop=True)\n",
    "user_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f6324-9521-4f7f-a021-1835c1634861",
   "metadata": {},
   "source": [
    "Now let us merge it with all the ratings of the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ab4a4-e296-4850-a7e1-ef5aa4622cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_users_ratings = user_corr.merge(ratings[[\"userId\", \"movieId\", \"rating\"]], how=\"inner\")\n",
    "top_users_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed62568-aca8-4369-92ce-7e668579d8cf",
   "metadata": {},
   "source": [
    "We can now create ratings that are weighted with respect to the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123700d-ecb3-499d-9d53-18c05d2eca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_users_ratings[\"weighted_rating\"] = top_users_ratings[\"correlation\"] * top_users_ratings[\"rating\"]\n",
    "top_users_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996c02e-d33e-4101-8bdc-d0e9874276b7",
   "metadata": {},
   "source": [
    "For each movie, we can now take the average of the weighted ratings to get a final rating for all the movies (as recommendation for the selectd random user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ee5cb-f0fd-4a73-be5a-041bd12e4040",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_df = top_users_ratings.groupby(\"movieId\").agg({\"weighted_rating\": \"mean\"}).sort_values(by = \"weighted_rating\", ascending = False)\n",
    "recommendation_df = recommendation_df.reset_index()\n",
    "recommendation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea747222-4d6b-43d5-8b50-aad0900ad5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_to_be_recommended = recommendation_df.merge(metadata[[\"id\", \"title\"]], left_on=\"movieId\", right_on=\"id\").drop(columns=[\"id\"])\n",
    "movies_to_be_recommended = movies_to_be_recommended.head()\n",
    "movies_to_be_recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd0fb9-f4aa-422b-8430-13e298b94dc1",
   "metadata": {},
   "source": [
    "We can now put it all together into a recommender function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112f80d-5539-4791-b9b9-e545ea8f1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_based_recommender(input_user, user_movie_df, rate_ratio=0.70, num_recommendations=5):\n",
    "    # Creating a list of movies the input user have rated\n",
    "    input_user_df = user_movie_df[user_movie_df.index == input_user]\n",
    "    input_user_movies_watched = input_user_df.columns[input_user_df.notna().any()].tolist()\n",
    "\n",
    "    # Creating a dataframe with the user rating of the movies the input user have rated\n",
    "    movies_watched_df = user_movie_df[input_user_movies_watched]\n",
    "\n",
    "    # Counting how many movies other users have rated that the input user have also rated\n",
    "    user_movie_count = movies_watched_df.T.notnull().sum()\n",
    "    user_movie_count = user_movie_count.reset_index()\n",
    "    user_movie_count.columns = [\"userId\", \"movie_count\"]\n",
    "    \n",
    "    # Selecting similar users over based on a rating similarity count ratio threshold\n",
    "    user_same_movies = user_movie_count[user_movie_count[\"movie_count\"] > (len(input_user_movies_watched)*rate_ratio)][\"userId\"]\n",
    "\n",
    "    # Creating a correlation matrix based on ratings\n",
    "    final_df = movies_watched_df[movies_watched_df.index.isin(user_same_movies)]\n",
    "    corr_df = final_df.T.corr()\n",
    "\n",
    "    # Created top correlated users\n",
    "    user_corr = corr_df[input_user].reset_index()\n",
    "    user_corr = user_corr.rename(columns={input_user: 'correlation'})\n",
    "    user_corr = user_corr.sort_values(by=\"correlation\", ascending=False)\n",
    "    user_corr = user_corr.loc[user_corr[\"userId\"] != input_user]\n",
    "    user_corr = user_corr.reset_index(drop=True)\n",
    "\n",
    "    # Creating correlated weighting of rating\n",
    "    top_users_ratings = user_corr.merge(ratings[[\"userId\", \"movieId\", \"rating\"]], how=\"inner\")\n",
    "    top_users_ratings[\"weighted_rating\"] = top_users_ratings[\"correlation\"] * top_users_ratings[\"rating\"]\n",
    "\n",
    "    # Creating a recommendation dataframe\n",
    "    recommendation_df = top_users_ratings.groupby(\"movieId\").agg({\"weighted_rating\": \"mean\"}).sort_values(by = \"weighted_rating\", ascending = False)\n",
    "    recommendation_df = recommendation_df.reset_index()\n",
    "\n",
    "    # Creating the final recommendations\n",
    "    movies_to_be_recommended = recommendation_df.merge(metadata[[\"id\", \"title\"]], left_on=\"movieId\", right_on=\"id\").drop(columns=[\"id\"])\n",
    "    movies_to_be_recommended = movies_to_be_recommended.head(num_recommendations)\n",
    "\n",
    "    return movies_to_be_recommended[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209256ef-a096-4c0b-b273-7bea4bb1818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa9240-f19b-4aff-b46a-2075f36c4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_based_recommender(455, user_movie_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9fc120-1624-46af-806a-7ba0c989b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_user_movies_watched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86092f92-13af-4522-8427-23785d57cffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
