{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df6c5c1d-1581-417f-992a-7cd2454a2c43",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "In this notebook, we will look at transfer learning. We saw how long it might take to train a model for classifying cats and dogs from the ground up. Let us instead try to utilize an existing model already trained for image classification to improve our classification of cats and dogs. Throughout this notebook, we will use the `VGG16` model for such, but there are many other newer and better models, of course. \n",
    "\n",
    "The code is heavily inspired by the Chapter 8 notebook from the book \"Deep learning with Python\" by Fran√ßois Chollet, available [here](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter08_intro-to-dl-for-computer-vision.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648fe4ce-b2d9-4a35-ad32-1d359f6a3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4fad2d-edc4-491d-9e74-23e29ce73545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    \"dogs_vs_cats_tiny/train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    \"dogs_vs_cats_tiny/validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    \"dogs_vs_cats_tiny/test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923dfae-ddee-4b19-9eae-7ad1279d4039",
   "metadata": {},
   "source": [
    "## Feature extraction once\n",
    "\n",
    "We fill first define or load a version of the VGG16 mode. Note, the first time, the code is run, the VGG16 model is downloaded. The `weights` argument just specify the particular version we want and the `include_top?` that we will not include the top dense layers as we want to add our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab05df-996d-4177-8a4c-3c23315e0aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(180, 180, 3))\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107d6af-24a5-4bac-8cd3-2c0da5313ed5",
   "metadata": {},
   "source": [
    "Let us now pass our images through this convolutional base to extract a new feature representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea925c-ed5f-4637-a3ca-c2299eb89f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for images, labels in dataset:\n",
    "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
    "        features = conv_base.predict(preprocessed_images)\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
    "\n",
    "train_features, train_labels =  get_features_and_labels(train_dataset)\n",
    "val_features, val_labels =  get_features_and_labels(validation_dataset)\n",
    "test_features, test_labels =  get_features_and_labels(test_dataset)\n",
    "\n",
    "print(\"Shape of training features:\", train_features.shape)\n",
    "print(\"Shape of training labels:\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f1916-8acd-4044-baa7-7bf7486759da",
   "metadata": {},
   "source": [
    "We can now train a dense deep neural network on this new feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24187e9-a12d-4358-b08d-81c5c1422334",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(5, 5, 512)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27fb8f8-3da6-4f83-b8d8-33c9260c16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc86b11-39f7-49bc-bb64-8b6fc4436251",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=\"feature_extraction_once.keras\",\n",
    "      save_best_only=True,\n",
    "      monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674098ab-1033-4171-9420-67751f216305",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_dense.fit(\n",
    "    train_features, train_labels,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009e675-fe80-4cdc-a68c-1f7669690443",
   "metadata": {},
   "source": [
    "Note how fast it went, as we only have to train a few dense layers! Let us plot the training and validation accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210135d-8b22-4a47-b268-173afe529c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17f703-7073-47f2-90f3-f9c1650114ed",
   "metadata": {},
   "source": [
    "We quite quickly got good performance and started to overfit! This is probably also due to the fact that the cats and dog images we are training on here, were likely also part of the ImageNet dataset that VGG16 was trained on, so the base model might already have seen these training images and learned them. Still we get a really god performance also on the testset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab678a-bf21-48a0-92f9-19123728ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\"feature_extraction_once.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_features, test_labels)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6b1c2-3ee2-4875-975f-5d73c04ebfe4",
   "metadata": {},
   "source": [
    "## Feature extraction integrated in the learning step\n",
    "\n",
    "We will now set up a complete model with the convolutional base together with a new classifier of dense layers on top. We will then freeze the parameters of the convolutional base and train the entire model on our data. In this case, we also have the ability to add augmentation first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe36e0-3cf1-439a-b959-6d20b9c24821",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(180, 180, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c2e3a-98c0-4d3a-8a44-abc1cc98c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4506f-1630-4459-8c61-91a26b9f6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = keras.applications.vgg16.preprocess_input(x)\n",
    "x = conv_base(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_dense_learning = keras.Model(inputs, outputs)\n",
    "model_dense_learning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb75670-666f-47e8-8f5e-475bf6511203",
   "metadata": {},
   "source": [
    "Let us freeze the convolutional base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2782b4-63a9-4f2c-9b24-e0c4d9fbee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False\n",
    "model_dense_learning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70531af1-3899-4c28-ad44-74c8ec3ec71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense_learning.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63edf5a7-fe1d-4cb9-b606-c72cd5111241",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=\"feature_extraction_learning.keras\",\n",
    "      save_best_only=True,\n",
    "      monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2814d6c8-51af-4d30-8ac4-113bd0117010",
   "metadata": {},
   "source": [
    "We can now train the model. However, this will take quite some time, as we have to parse the training data through the entire model and we also have additional data augmenatin now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bccb486-ea4e-468a-8559-dd8e770eb72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_dense_learning.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d8727-80a6-458c-89aa-cbc4ede0db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc2d03-c599-4d77-894e-21d873b4c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\"feature_extraction_learning.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476c36f-c039-47ca-9049-ecff3da4294c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dfd1898-92bc-4747-9cc7-de5f3bc03f3e",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "We can now take the model just trained and un freeze a few of the parameters in the convolutional base, and then re-train the entire model. Let us first unfreeze the mentioned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bffff79-8ff8-45ad-925c-1eda9f28f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "for layer in conv_base.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f6b76-5d3c-4f5a-8556-b89a83ba586f",
   "metadata": {},
   "source": [
    "Before we can retrain the model, we need to compile it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ccf1a-68d4-4754-a479-c2d0257034e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense_learning.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cba462-5146-4902-9d03-a8d1c43650ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"fine_tuning.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32361006-7fc6-49b8-8fd1-7b093a05c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_dense_learning.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d45d361-e82c-47e2-9aef-757e91a33bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7850e-2bdb-4e9e-820a-2ecd3e1f9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"fine_tuning.keras\")\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce51784-0006-4c18-9a6f-bc5a226be7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
