{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5499e3da-49fb-40eb-ac1b-472cf868e000",
   "metadata": {},
   "source": [
    "# Exercises in neural network and deep learning II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbcd5e5-5836-48d4-bce6-bb9f1e72db14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d345f40-333b-4d72-b05c-ec4068174d62",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "In this exercise, we will use the titanic dataset to build a neural network classifier for the target variable `Survived`. In this exercise, all the necessary steps are broken down to small individual task you should do. So, do the following tasks:\n",
    "\n",
    "1. Load in the titanic dataset (on Moodle as \"titanic_survival_data.csv\"), select the columns \"Pclass\", \"Sex\", \"Age\", and \"Fare\" for the feature set X and \"Survived\" for the target variable y.\n",
    "2. For the columns \"Age\" and \"Fare\", replace the missing values with the column's mean.\n",
    "3. Turn the variables \"Pclass\" and \"Sex\" into dummy variables.\n",
    "4. Do a train-test split of the data\n",
    "5. Scale the X training dataset, using the standard scaler.\n",
    "6. Transform the X test dataset with the same scaler fitted in task 5.\n",
    "7. Turn the training and test data (both Xs and ys) into numpy arrays using the method `.to_numpy()`.\n",
    "8. Create a neural network model with two hidden dense layers with 16 neurons in each and the `relu` activation function. The final output layer should have one neuron and the `sigmoid` activation function.\n",
    "9. Train the model for 50 epochs with a batch size of 64. Instead of creating a validation set before training the model, we can give it as an argument to the fit method, that it should set aside, let's say, 20% of the training data for validation. Do this by providing the argument `validation_split=0.2` to the fit method.\n",
    "10. Plot the training and validation loss and decide on a best number of epochs.\n",
    "11. Rebuild and retrain the model for the number of epochs decided in task 10 above.\n",
    "12. Evaluate the model on the test dataset to get the test accuracy.\n",
    "13. Calculate precision, recall, and f1-score for the model on the test dataset.\n",
    "14. Plot the Confusion matrix for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce3ca37-39dc-4009-8437-eacf43051867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "130f32d2-1eca-4f9b-8a1a-be0e85f447bf",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Based on the **adult dataset**, build a neural network classifier for the target variable `income`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3587ca-8799-4d16-a74d-fb07eaa8d053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bde7a13-0e1a-44c0-8d71-e9f2e0d4fa79",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Based on the **AmesHousing dataset** from the class on regression, make a deep learning regression model to predict the sales price variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2a0b2-16b6-47fa-bc41-b6e86ff194f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
