{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda9ea22-0b7e-4d78-8cab-8cb544465fba",
   "metadata": {},
   "source": [
    "# Overfitting and regularization\n",
    "\n",
    "In this notebook we will look at various techniques to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9094ac5b-5344-4e00-b9f2-382f034c421a",
   "metadata": {},
   "source": [
    "We will start out by using the IMDB binary classification example from last time (ch 4 of the book). So let set that up and train the baseline model from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f0f49c-fc03-47b4-a7b3-472d467d3a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabfdb70-1590-4972-a98a-2796fa1a4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4d79d-6182-4d20-b24b-1c31eaa76d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results\n",
    "    \n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12633d75-f68e-48ca-bac7-b24247e51b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f9499-1bdd-4eb2-bc01-c4a4e0344d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eff9bb-9921-4288-bb2f-fa50b7c24704",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b515499a-f59a-457b-96cb-18bfed7b5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c8808-d6c7-45b0-ae8f-3985496b2d24",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b05501-b862-4e6d-a9a2-ef09b1c4e230",
   "metadata": {},
   "source": [
    "We can manually inspect the loss graph and stop the model when it reaches it min loss (here 4 epoch) or we can more systematically keep the model with the lowest validation loss using the ModelCheckpoint callback from Keras. First we set up callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75145203-92fb-4ee8-87b5-6361aefbe67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"imbd_overfitting_ex.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9fb4e-f2b4-49a4-9d4d-9170618b3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a8d18-75d7-402e-b581-3244aa52a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e313ac-bb92-4d8f-bb53-41b785aacd6d",
   "metadata": {},
   "source": [
    "Now we can retreive the best model and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b3877-a893-4ece-8dd4-64fd8720901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\"imbd_overfitting_ex.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ca690-44ff-4639-affe-7892f5fbf10e",
   "metadata": {},
   "source": [
    "This corresponds to the best model we could have manually gotten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ac6ae-de2e-4133-be66-eea6ded985cb",
   "metadata": {},
   "source": [
    "## Weight regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7d40c-db08-4bee-8b6e-b98b35e505c0",
   "metadata": {},
   "source": [
    "Let us try to add weight regularization in the form of L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c31d7-50f8-4b04-b5db-5ca95d97bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(0.003)),\n",
    "    layers.Dense(16, activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(0.003)),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2ba6c-fded-45db-8176-0ba06a2de904",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e919070-70f2-4dc5-b5a2-97797bfff871",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc70c6-1740-40ac-8739-02b1b2c35917",
   "metadata": {},
   "source": [
    "We see that the model starts to overfit later much later and to a less degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b1937-d9da-4d5a-a3a3-7a0488c25c6c",
   "metadata": {},
   "source": [
    "We refit it on 6 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7b302-1f74-4cfe-a2d9-2e73b2c0a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(0.003)),\n",
    "    layers.Dense(16, activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(0.003)),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=6,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c106bbe-00fa-4a37-856b-1093524c037b",
   "metadata": {},
   "source": [
    "And test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d5a56-b9f1-41f4-8d12-569b5ea48868",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cb7a3-d656-4886-8f98-bbce53c28c5b",
   "metadata": {},
   "source": [
    "In this case, we did not managed to get a better model though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eca448-ed70-4959-ae58-aed90692490f",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22668a67-1674-4591-b96c-36c89816440f",
   "metadata": {},
   "source": [
    "We can also try to add dropout after each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c312267-ed5e-40e7-9539-2d223d12a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1de73-52bf-4b8a-bc31-7895b2beaaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ffa55-1325-4e5c-9b1a-1db98fc50aff",
   "metadata": {},
   "source": [
    "Again we see a later and less extreme overfitting. Let us retrain the model on 7 epochs and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac5381-7845-41cd-9a24-3db092f19419",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=7,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4238fd9-465c-4008-a5a9-91a16f21cea6",
   "metadata": {},
   "source": [
    "Again, we did not manage to get a much better model in the end. It might suggest that overfitting is not as much of a problem for this model to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f9048-8408-4951-81d8-fe15cef177ef",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17137967-3f1b-48d0-b5df-767b59ab2b83",
   "metadata": {},
   "source": [
    "Data augmentation is something we usually do for computer vision models, so we need to look at the example of cats vs dogs again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1937a-5792-445d-8584-5acdc954f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    \"dogs_vs_cats_tiny/train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    \"dogs_vs_cats_tiny/validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    \"dogs_vs_cats_tiny/test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c004f911-ddc1-4056-b767-6f1bf7e5d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_and_d_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(180, 180, 3)),\n",
    "        layers.Rescaling(1./255),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(256, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "c_and_d_model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50454938-136b-416f-a817-a2da21e354c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = c_and_d_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=8,  # I have already checked when to stop...\n",
    "    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb99c68-012f-4036-88ae-ca2a6b6a9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = c_and_d_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98231f15-5d69-406b-84e4-177a8484fcf7",
   "metadata": {},
   "source": [
    "To do data augmentation in keras we can add data augmentation layers to the start of our model. Here is an example of such data augmentation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91037694-d3f1-4803-ab3f-f97d8786ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),   # Applies horizontal flipping to a random 50% of the images that go through it\n",
    "        layers.RandomRotation(0.1),        # Rotates the input images by a random value in the range [–10%, +10%] (these are\n",
    "                                           # fractions of a full circle—in degrees, the range would be [–36 degrees, +36 degrees])\n",
    "        layers.RandomZoom(0.2),            # Zooms in or out of the image by a random factor in the range [-20%, +20%]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da80b9-dc33-4656-86ea-2d857d1b72f5",
   "metadata": {},
   "source": [
    "Here is an example of how the images will look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346dfd3-c7f8-493e-95b5-7a0a620bd126",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81b214-dc78-4d2c-a48e-16c817a59a09",
   "metadata": {},
   "source": [
    "Let us retrain the cat vs dog model again with this data augmenation layers at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f628bdf-8da1-4341-a9fe-c6bc97b0ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_and_d_model_w_aug = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(180, 180, 3)),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.Rescaling(1./255),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(256, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "c_and_d_model_w_aug.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df18688-90b3-4317-95f9-af86f29a932d",
   "metadata": {},
   "source": [
    "As we now expect less overfitting we will also train our model for a longer time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2733867-23de-46dd-9cfc-07a8c92d00bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = c_and_d_model_w_aug.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531fedd-be33-4e65-8b3a-5724c3900bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1191917-b23a-4392-9dbe-b8ad0c3f9daf",
   "metadata": {},
   "source": [
    "Before we started to overfit around 8 epochs and now we haven't quite overfitted yet at 30 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9331aa-e687-4b53-a9e0-5ecbfdcf87bf",
   "metadata": {},
   "source": [
    "Let us now evaluate it on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d83b7-3365-445a-a9fc-619bc2e11278",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = c_and_d_model_w_aug.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59598cd1-a16d-4056-8c57-62e3ec699c1a",
   "metadata": {},
   "source": [
    "We see a clear improvement here! And if we made sure fit until the point just before overfitting, we might be able to do even better..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca3bf0-0b23-416c-8bf9-f7f2266ffd06",
   "metadata": {},
   "source": [
    "Now let us add it all together. That is, let us train the model with data augmentation, dropout and early stopping using callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99844abc-991e-4fd7-8305-a8c8faba0332",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"cat_vs_dogs_final.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63a9fa-e277-4510-9be8-27078c1f40a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_and_d_model_final = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(180, 180, 3)),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.Rescaling(1./255),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(256, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "c_and_d_model_final.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a72ea-5b23-4f7d-a949-4a843c5fae74",
   "metadata": {},
   "source": [
    "As we expect much less overfitting we will also train for even longer now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d270282-51a2-4bfb-b652-802f8fe46571",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = c_and_d_model_final.fit(\n",
    "    train_dataset,\n",
    "    epochs=60,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d37725-7874-44e7-b06c-886352dcc927",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1ddb2-d74a-49d0-8f37-4cd920666de0",
   "metadata": {},
   "source": [
    "We see that we do not overfit until we reach close to 60 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e9bec5-1f94-4a8e-adeb-80bf57c83ee1",
   "metadata": {},
   "source": [
    "Let us now test the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905785d-39d3-46a3-a3f9-394ad797abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\"cat_vs_dogs_final.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa88e7-51ca-43b2-b93b-7877714c8e7f",
   "metadata": {},
   "source": [
    "We now see a significant improvement over the baseline model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eddb7a2-6dbb-492e-89b2-1665d7aba43e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
